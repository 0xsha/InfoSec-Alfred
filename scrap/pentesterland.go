package scrap

/*
This file is part of Alfred
(c) 2020 - 0xSha.io
*/

import (
	"github.com/PuerkitoBio/goquery"
	"github.com/gocolly/colly"
	"github.com/jinzhu/gorm"
	"log"
	"strings"
)

func FetchPentesterLand() (map[int][]string , error) {
	// Instantiate default collector
	c := colly.NewCollector(
		colly.MaxDepth(1),

		colly.AllowedDomains("pentester.land"),
	)

	pentesterLandArr := make(map[int][]string)


	// On every a element
	c.OnHTML("#bug-bounty-writeups-published-in-2020", func(e *colly.HTMLElement) {

		e.DOM.Next().Find("a").Each(func(i int, selection *goquery.Selection) {

			link,_ := selection.Attr("href")
			title := selection.Text()

			if i >= 18{
				return
			}

			// filters and ten records
			if strings.Contains(selection.Text(),"@") || strings.Contains(link,"twitter"){
				return
			}

			pentesterLandArr[i] = []string{title,link}

			log.Println(title)
			log.Println(link)


		})
	})

	// Before making a request print "Visiting ..."
	c.OnRequest(func(r *colly.Request) {
		log.Println("Visiting", r.URL.String())
	})

	// Start scraping
	err := c.Visit("https://pentester.land/list-of-bug-bounty-writeups.html")

	if err != nil {
		return nil,err
	}

	return pentesterLandArr,nil
}

func WritePentesterLandJobsToDB(newsArr map[int][]string,entity Entity, db *gorm.DB) (int,error) {

	totalFound := 0

	for _,key := range newsArr{

		entity.Title = key[0]
		entity.URL =  key[1]
		entity.Source = "PentesterLand"

		if err := db.Create(&entity).Error; err !=nil {
			log.Println(err)
		}else {
			totalFound++
		}
		entity.ID++
	}
	return totalFound,nil
}
